{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook by:\n",
    "\n",
    "* Lorenzo Pannacci 1948926\n",
    "* Stefano Rinaldi 1945551"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Startup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# LIBRARIES DOWNLOAD #\n",
    "######################\n",
    "\n",
    "install_packages = True\n",
    "if install_packages:\n",
    "    %pip install numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# LIBRARIES IMPORT #\n",
    "####################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Recommendation sytem \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - top 10 films"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**taking into considetation only click counts**\n",
    "\n",
    "This is what I am going to do step by step: \n",
    "\n",
    "1) Load the data <br>\n",
    "2) Fix the dataset (if necessary) <br>\n",
    "3) Group by user and movie <br>\n",
    "4) Count the clicks <br>\n",
    "5) Sort the data by click count for each user.<br>\n",
    "6) Extract the top 10 movies for each user and take title and genre.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1)/2) load the data and fix the dataset\n",
    "dataset = pd.read_csv('vodclickstream_uk_movies_03.csv')\n",
    "\n",
    "#clean it\n",
    "dataset = dataset.drop(\"Unnamed: 0\", axis=1)\n",
    "\n",
    "# Convert datetime column to a date\n",
    "dataset.datetime = pd.to_datetime(dataset.datetime)\n",
    "\n",
    "# Convert release_date column to a date\n",
    "dataset.release_date = pd.to_datetime(dataset.release_date, errors=\"coerce\")\n",
    "\n",
    "# Extract genres column to a new column genre_list that includes a list with all genres\n",
    "dataset[\"genres_list\"] = dataset.genres.apply(lambda row: [word.strip() for word in row.split(\",\")])\n",
    "\n",
    "#retrieve all unique film genres\n",
    "unique_genres = set()\n",
    "\n",
    "dataset[\"genres_list\"].apply(lambda row: [unique_genres.add(value) for value in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3)/4)/5)/6) group by user and movie, count clicks and sort them\n",
    "\n",
    "#create a new dataframe with the information I need\n",
    "users = dataset[[\"genres_list\", \"user_id\",\"title\", \"genres\"]]\n",
    "\n",
    "#group user and movie and count the click\n",
    "user_movie_counts =users.groupby(['user_id', 'title']).size().reset_index(name='click_count')\n",
    "\n",
    "#sort them\n",
    "user_movie_counts = user_movie_counts.sort_values(by=['user_id', 'click_count'], ascending=[True, False])\n",
    "\n",
    "#take the top 10 movies for each user\n",
    "user_movie_counts = user_movie_counts.groupby('user_id').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>title</th>\n",
       "      <th>click_count</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00004e2862</td>\n",
       "      <td>Hannibal</td>\n",
       "      <td>1</td>\n",
       "      <td>Crime, Drama, Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000052a0a0</td>\n",
       "      <td>Looper</td>\n",
       "      <td>9</td>\n",
       "      <td>Action, Drama, Sci-Fi, Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>000052a0a0</td>\n",
       "      <td>Frailty</td>\n",
       "      <td>3</td>\n",
       "      <td>Crime, Drama, Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>000052a0a0</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>3</td>\n",
       "      <td>Adventure, Comedy, Family, Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>000052a0a0</td>\n",
       "      <td>Resident Evil</td>\n",
       "      <td>2</td>\n",
       "      <td>Action, Horror, Sci-Fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613980</th>\n",
       "      <td>fffeac83be</td>\n",
       "      <td>To the Bone</td>\n",
       "      <td>1</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613981</th>\n",
       "      <td>fffeac83be</td>\n",
       "      <td>True Story</td>\n",
       "      <td>1</td>\n",
       "      <td>Crime, Drama, Mystery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613982</th>\n",
       "      <td>ffff2c5f9e</td>\n",
       "      <td>Forks Over Knives</td>\n",
       "      <td>1</td>\n",
       "      <td>Documentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613983</th>\n",
       "      <td>ffff2c5f9e</td>\n",
       "      <td>Hot Fuzz</td>\n",
       "      <td>1</td>\n",
       "      <td>Action, Comedy, Mystery, Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613984</th>\n",
       "      <td>ffffd36adf</td>\n",
       "      <td>Dead Man Walking</td>\n",
       "      <td>1</td>\n",
       "      <td>Crime, Drama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>447114 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           user_id              title  click_count  \\\n",
       "0       00004e2862           Hannibal            1   \n",
       "1       000052a0a0             Looper            9   \n",
       "10      000052a0a0            Frailty            3   \n",
       "13      000052a0a0            Jumanji            3   \n",
       "16      000052a0a0      Resident Evil            2   \n",
       "...            ...                ...          ...   \n",
       "613980  fffeac83be        To the Bone            1   \n",
       "613981  fffeac83be         True Story            1   \n",
       "613982  ffff2c5f9e  Forks Over Knives            1   \n",
       "613983  ffff2c5f9e           Hot Fuzz            1   \n",
       "613984  ffffd36adf   Dead Man Walking            1   \n",
       "\n",
       "                                    genres  \n",
       "0                   Crime, Drama, Thriller  \n",
       "1          Action, Drama, Sci-Fi, Thriller  \n",
       "10                  Crime, Drama, Thriller  \n",
       "13      Adventure, Comedy, Family, Fantasy  \n",
       "16                  Action, Horror, Sci-Fi  \n",
       "...                                    ...  \n",
       "613980                               Drama  \n",
       "613981               Crime, Drama, Mystery  \n",
       "613982                         Documentary  \n",
       "613983   Action, Comedy, Mystery, Thriller  \n",
       "613984                        Crime, Drama  \n",
       "\n",
       "[447114 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#return a new dataframe with the top 10 movies for each user;\n",
    "# I give as information the user, the titple and the genre of the film (as well as the click count)\n",
    "#other pieces of information are not asked\n",
    "\n",
    "result = pd.merge(user_movie_counts, users[['user_id', 'title', 'genres']], on=['user_id', 'title'], how='left')\n",
    "\n",
    "result['genres'] = result['genres'].astype(str)\n",
    "\n",
    "#remove duplicates. I have to do this because the previous code gives the right answer, but\n",
    "#it creates n rows for the x value of the click count.\n",
    "#so if a film has 9 clicks, it creates 9 times that rows.\n",
    "result = result.drop_duplicates()\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>title</th>\n",
       "      <th>click_count</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70879</th>\n",
       "      <td>1df8e21154</td>\n",
       "      <td>Beastly</td>\n",
       "      <td>5</td>\n",
       "      <td>Drama, Fantasy, Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70884</th>\n",
       "      <td>1df8e21154</td>\n",
       "      <td>Titanic</td>\n",
       "      <td>4</td>\n",
       "      <td>Drama, Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70888</th>\n",
       "      <td>1df8e21154</td>\n",
       "      <td>Addicted</td>\n",
       "      <td>3</td>\n",
       "      <td>Drama, Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70891</th>\n",
       "      <td>1df8e21154</td>\n",
       "      <td>Match Point</td>\n",
       "      <td>3</td>\n",
       "      <td>Drama, Romance, Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70894</th>\n",
       "      <td>1df8e21154</td>\n",
       "      <td>My Last Day without You</td>\n",
       "      <td>3</td>\n",
       "      <td>Comedy, Drama, Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70897</th>\n",
       "      <td>1df8e21154</td>\n",
       "      <td>The Red Thread</td>\n",
       "      <td>3</td>\n",
       "      <td>Drama, Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70900</th>\n",
       "      <td>1df8e21154</td>\n",
       "      <td>After Earth</td>\n",
       "      <td>2</td>\n",
       "      <td>Action, Adventure, Sci-Fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70902</th>\n",
       "      <td>1df8e21154</td>\n",
       "      <td>Boomerang</td>\n",
       "      <td>2</td>\n",
       "      <td>Comedy, Drama, Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70904</th>\n",
       "      <td>1df8e21154</td>\n",
       "      <td>Diary of a Mad Black Woman</td>\n",
       "      <td>2</td>\n",
       "      <td>Comedy, Drama, Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70906</th>\n",
       "      <td>1df8e21154</td>\n",
       "      <td>Lift Me Up</td>\n",
       "      <td>2</td>\n",
       "      <td>Family</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          user_id                       title  click_count  \\\n",
       "70879  1df8e21154                     Beastly            5   \n",
       "70884  1df8e21154                     Titanic            4   \n",
       "70888  1df8e21154                    Addicted            3   \n",
       "70891  1df8e21154                 Match Point            3   \n",
       "70894  1df8e21154     My Last Day without You            3   \n",
       "70897  1df8e21154              The Red Thread            3   \n",
       "70900  1df8e21154                 After Earth            2   \n",
       "70902  1df8e21154                   Boomerang            2   \n",
       "70904  1df8e21154  Diary of a Mad Black Woman            2   \n",
       "70906  1df8e21154                  Lift Me Up            2   \n",
       "\n",
       "                          genres  \n",
       "70879    Drama, Fantasy, Romance  \n",
       "70884             Drama, Romance  \n",
       "70888            Drama, Thriller  \n",
       "70891   Drama, Romance, Thriller  \n",
       "70894     Comedy, Drama, Romance  \n",
       "70897             Drama, Romance  \n",
       "70900  Action, Adventure, Sci-Fi  \n",
       "70902     Comedy, Drama, Romance  \n",
       "70904     Comedy, Drama, Romance  \n",
       "70906                     Family  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#here you can retrieve the result for a specific user\n",
    "### CHANGE THE USER HERE TO THE DESIRED ONE\n",
    "input_user = \"1df8e21154\"\n",
    "\n",
    "specific_user = result[result['user_id'] == input_user]\n",
    "\n",
    "specific_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**little comment** <br>\n",
    "here we can see the result for a specific user (and before also for all users together). Result is ordered by click count. In this case, we are taking into consideration all clicks and films and users. Even if a click has a duration of 0, it is taken into consideration as the user showed interest to the film and so is useful for both building the next hashing function and for counting the film with most clicks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Minhash Signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what I am going to do:\n",
    "\n",
    "1) **group users and genres**:<br>\n",
    "I create a dictionary in which the keys are the *user_id* and the values the *genres* associated to each user. repetition of genres are allowed as I need to understand the pattern of a user, so the genre they like the most. <br>\n",
    "2) **create hash function**: <br>\n",
    "I create the hash function by creating my own code as it is **not permitted** to use **already implemented functions**.\n",
    "2) **calculate the minhash and buckets**: <br>\n",
    "Now I find the minhash for genres and then I store users based on their minhash. Users with the same minhash signature are grouped into the same bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Here I create a 2 columns dataframe. one with the users_id and one with the genres. \n",
    "#each row has only one genre. this will be needed for creating a dictionary with users as keys and genres as values\n",
    "\n",
    "small_dataset = dataset[[\"genres\",\"user_id\"]]\n",
    "\n",
    "exploded_genres = small_dataset.assign(genres=dataset['genres'].str.split(',')).explode('genres').reset_index(drop=True)\n",
    "\n",
    "exploded_genres['genres'] = exploded_genres['genres'].str.strip()\n",
    "\n",
    "# Duplicate the 'user_id' column to match the number of rows in the exploded DataFrame\n",
    "genres_with_user = pd.concat([exploded_genres['user_id'], exploded_genres['genres']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) create a dictionary where I have as keys the user_id and as values all genres related to it\n",
    "\n",
    "user_genres_dict = {}\n",
    "\n",
    "# Iterare sul dataframe per riempire il dizionario\n",
    "for index, row in genres_with_user.iterrows():\n",
    "    user_id = row['user_id']\n",
    "    genre = row['genres']\n",
    "\n",
    "    #if user_id already exist, append \n",
    "    if user_id in user_genres_dict:\n",
    "        user_genres_dict[user_id].append(genre)\n",
    "    #if user_id does not exist, create the key\n",
    "    else:\n",
    "        user_genres_dict[user_id] = [genre]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2) create my hashing functions:\n",
    "\n",
    "def hash1(input_string):\n",
    "    ##random number\n",
    "    hash_value = 13\n",
    "    \n",
    "    for char in input_string:\n",
    "        #other random number + univocate code for each character\n",
    "        # divided by % (2**32) so that the number is not too big \n",
    "        hash_value = (hash_value * 17 + ord(char)) % (2**32)\n",
    "\n",
    "    return hash_value\n",
    "\n",
    "def hash2(data):\n",
    "    \n",
    "    hash_value = sum(ord(char) for char in str(data))\n",
    "\n",
    "    return hash_value\n",
    "\n",
    "def hash3(data):\n",
    "    hash_value = 3\n",
    "\n",
    "    for char in str(data):\n",
    "        hash_value = (hash_value * 13 + ord(char)) % (2**32)\n",
    "\n",
    "    return hash_value\n",
    "\n",
    "def hash4(data):\n",
    "    hash_value = 2\n",
    "\n",
    "    for char in str(data):\n",
    "        hash_value = (hash_value * 17) ^ ord(char)\n",
    "        hash_value = hash_value % (2**32)\n",
    "\n",
    "    return hash_value\n",
    "\n",
    "def hash5(data):\n",
    "    hash_value = 1\n",
    "    \n",
    "    for char in str(data):\n",
    "        hash_value = hash_value * ord(char)\n",
    "    \n",
    "    return hash_value\n",
    "\n",
    "def hash6(data):\n",
    "    hash_value = 13\n",
    "\n",
    "    for char in str(data):\n",
    "        hash_value = (hash_value + ord(char)) \n",
    "        hash_value = hash_value % (2**32)\n",
    "\n",
    "    return hash_value\n",
    "\n",
    "def hash7(data):\n",
    "    hash_value = 88\n",
    "    \n",
    "    for char in str(data):\n",
    "        hash_value = sum(ord(char) for char in str(data))\n",
    "        hash_value = hash_value + ord(char)\n",
    "        hash_value = hash_value % (2**32)\n",
    "\n",
    "    return hash_value\n",
    "\n",
    "def hash8(data):\n",
    "    hash_value = 12\n",
    "\n",
    "    for char in str(data):\n",
    "        hash_value = (hash_value + ord(char)) % ord(char) \n",
    "\n",
    "    return hash_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3) apply the function over the dictionary (user and genre)\n",
    "def minhash(user_genres_dict, hash_functions):\n",
    "    #create a dictionary to store users with the same interests into the same bucket\n",
    "    buckets = {}\n",
    "\n",
    "    #for every user in the dictionary\n",
    "    for user_id, genres in user_genres_dict.items():\n",
    "        #start mingash\n",
    "        #the first hash value encountered for each hash function will always be considered as the initial minimum.\n",
    "        signature = [float('inf')] * len(hash_functions)\n",
    "\n",
    "        #for each genre\n",
    "        for genre in genres:\n",
    "\n",
    "            #The enumerate function is used to get both the index (i) and the actual hash function (hash_func) from the list\n",
    "            for i, hash_func in enumerate(hash_functions):\n",
    "\n",
    "                #apply each hash function of the list I will gave to it\n",
    "                hash_value = hash_func(genre)\n",
    "                \n",
    "                #updates the minhash signature for the current hash function (signature[i]). \n",
    "                #it compares the existing value in the signature with the newly computed hash value (hash_value).\n",
    "                signature[i] = min(signature[i], hash_value)\n",
    "\n",
    "        #add the user to the corresponding bucket(s) based on the hash.\n",
    "        for hash_value in signature:\n",
    "            if hash_value not in buckets:\n",
    "                buckets[hash_value] = []\n",
    "            buckets[hash_value].append(user_id)\n",
    "\n",
    "    return buckets\n",
    "\n",
    "#my hashing functions\n",
    "hash_functions = [hash1, hash2, hash3, hash4, hash5, hash6, hash7, hash8]\n",
    "\n",
    "#my buckets\n",
    "user_buckets = minhash(user_genres_dict, hash_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Locality-Sensitive Hashing (LSH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will do a lot of things: <br>\n",
    "steps to follow: <br>\n",
    "1) **input the user** <br>\n",
    "input the user to which you wish to suggest 5 films. <br>\n",
    "2) **Identify the bucket(s)** <br>\n",
    "Find to which bucket(s) the user is stored <br>\n",
    "3) **retrieve the bucket** <br>\n",
    "Take the bucket with the less values (*user_id*) associated to it. I do this because I want to to take the bucket with the less users because if a bucket has a lot of users it means that is very general and may have some generic genres whereas if a bucket has less user_id, it means that they all share a common sense of film genres and so I can have a better and more specific result. <br>\n",
    "4) **Identify the two most similar users** <br>\n",
    "Now I compute the Jaccard Similarity between my input user and all the users present in that bucket. for doing so, I use also the previous created dictionary in which I have all users and all genres. Then I sort the users by the similarity and I take the top 2. <br>\n",
    "5) **suggest common films** <br>\n",
    "If these two users have any movies in common, recommend those movies in order based on the total number of clicks by these users. <br>\n",
    "6) **suggest other films** <br>\n",
    "If there are no common movies or less than 5 common movies, try to propose the most clicked movies by the most similar user first, followed by the other user. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1)\n",
    "#INPUT HERE THE USER TO WHICH YOU WANT TO FIND THE MOST SIMILAR USERS (and so suggest to them 5 films)\n",
    "user_id_input = \"9f3348702f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) I check in which bucket(s) the user is\n",
    "\n",
    "def find_user_buckets(user_id, buckets):\n",
    "    user_buckets = []\n",
    "\n",
    "    for hash_value, users in buckets.items():\n",
    "        if user_id in users:\n",
    "            user_buckets.append(hash_value)\n",
    "\n",
    "    return user_buckets\n",
    "\n",
    "found_buckets = find_user_buckets(user_id_input, user_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3) check the smallest bucket and take the values (user_id) associated to it\n",
    "\n",
    "keys_to_check = found_buckets \n",
    "\n",
    "# Initialize variables to keep track of the key with the minimum length and its corresponding length\n",
    "min_length_key = None\n",
    "min_length = float('inf')\n",
    "\n",
    "#for all keys\n",
    "for key in keys_to_check:\n",
    "    # Check if the key exists in the user_buckets dictionary\n",
    "    if key in user_buckets:\n",
    "        # Find the length of the list of users associated with the key\n",
    "        length_of_users = len(user_buckets[key])\n",
    "        \n",
    "        # Update the minimum length and key if the current length is smaller\n",
    "        if length_of_users < min_length:\n",
    "            min_length = length_of_users\n",
    "            min_length_key = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I take the smallest bucket to which the user appartains\n",
    "bucket_estrarre = min_length_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_values_for_key(dictionary, key_to_extract):\n",
    "    #extract values associated to the key\n",
    "    values = dictionary[key_to_extract]\n",
    "    return values\n",
    "\n",
    "\n",
    "values_del_bucket = extract_values_for_key(user_buckets, bucket_estrarre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>jaccard_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2704</th>\n",
       "      <td>bff0dc181e</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2103</th>\n",
       "      <td>1b9ca228c8</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2159</th>\n",
       "      <td>c0751e2150</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9386</th>\n",
       "      <td>3bb52799c4</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3556</th>\n",
       "      <td>cb848ec520</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9639</th>\n",
       "      <td>0116aa83d2</td>\n",
       "      <td>0.058824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11436</th>\n",
       "      <td>c08252bdf0</td>\n",
       "      <td>0.058824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>23e33addd6</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4027</th>\n",
       "      <td>a8b897b8b3</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9439</th>\n",
       "      <td>805e238545</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12189 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          user_id  jaccard_similarity\n",
       "2704   bff0dc181e            0.875000\n",
       "2103   1b9ca228c8            0.857143\n",
       "2159   c0751e2150            0.857143\n",
       "9386   3bb52799c4            0.750000\n",
       "3556   cb848ec520            0.750000\n",
       "...           ...                 ...\n",
       "9639   0116aa83d2            0.058824\n",
       "11436  c08252bdf0            0.058824\n",
       "609    23e33addd6            0.055556\n",
       "4027   a8b897b8b3            0.055556\n",
       "9439   805e238545            0.055556\n",
       "\n",
       "[12189 rows x 2 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4) find the two most similar users\n",
    "\n",
    "#jaccard similarity\n",
    "def calcola_jaccard_similarity(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union != 0 else 0.0\n",
    "\n",
    "#user_id to compare\n",
    "lista_user_id = values_del_bucket\n",
    "\n",
    "#genres of our input_user\n",
    "generi_input = set(user_genres_dict.get(user_id_input, []))\n",
    "\n",
    "jaccard_similarities = []\n",
    "\n",
    "for altro_user_id in lista_user_id:\n",
    "    generi_altro = set(user_genres_dict.get(altro_user_id, []))\n",
    "    jaccard_similarity = calcola_jaccard_similarity(generi_input, generi_altro)\n",
    "    jaccard_similarities.append({'user_id': altro_user_id, 'jaccard_similarity': jaccard_similarity})\n",
    "\n",
    "df_similarities = pd.DataFrame(jaccard_similarities)\n",
    "\n",
    "#sort the dataframe by 'jaccard_similarity'\n",
    "df_similarities = df_similarities.sort_values(by='jaccard_similarity', ascending=False)\n",
    "\n",
    "#drop eventual user_id duplicates\n",
    "df_similarities = df_similarities.drop_duplicates(subset='user_id')\n",
    "\n",
    "#drop the input user\n",
    "df_similarities = df_similarities[df_similarities['user_id'] != user_id_input]\n",
    "\n",
    "#sort the dataframe \n",
    "most_similar_users = df_similarities.sort_values(by='jaccard_similarity', ascending=False).groupby('user_id')\n",
    "\n",
    "most_similar_users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) If these two users have any movies in common, recommend those movies based on the total number of clicks by these users\n",
    "#extract common movies\n",
    "users = most_similar_users.user_id.head(2)\n",
    "\n",
    "user_1 =  users.iloc[0]\n",
    "user_2 =  users.iloc[1]\n",
    "\n",
    "# Filter data for each user\n",
    "user1_movies = user_movie_counts[user_movie_counts['user_id'] == user_1]\n",
    "user2_movies = user_movie_counts[user_movie_counts['user_id'] == user_2]\n",
    "\n",
    "#merge the dataframes such that I have all the movies together\n",
    "merged_users = pd.concat([user1_movies, user2_movies])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Icarus'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NOW, I suggest all movies that are in common and I order them by clicks count.\n",
    "\n",
    "user_groups = merged_users.groupby('user_id')['title'].apply(set)\n",
    "\n",
    "# Find the common titles between the two users\n",
    "common_titles = set.intersection(user_groups.iloc[0], user_groups.iloc[1])\n",
    "common_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>sum_click_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>374389</th>\n",
       "      <td>Icarus</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         title  sum_click_count\n",
       "374389  Icarus                3"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_df = merged_users[merged_users['title'].isin(common_titles)].copy()\n",
    "\n",
    "#count clicks for each film\n",
    "common_df['sum_click_count'] = common_df.groupby('title')['click_count'].transform('sum')\n",
    "\n",
    "#sort by the sum of cliks\n",
    "sorted_common_df = common_df.sort_values(by='sum_click_count', ascending=False)\n",
    "\n",
    "#films in common between the two users, drop dublicates\n",
    "sorted_common_df = sorted_common_df.drop_duplicates(subset='title')\n",
    "sorted_common_df = sorted_common_df[[\"title\",\"sum_click_count\"]]\n",
    "sorted_common_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "### here I do the following:\n",
    "#If the common films are 5, we are done.\n",
    "#if they are less than 5, I suggest the remaining ones from the user by greatest similarity.\n",
    "#if the user with greatest similarity has no more other films, I will pass to the second user.\n",
    "#ex: if the common films are 3, I will suggest only 2 films from the user with greatest similarity / the second user.\n",
    "\n",
    "#I check if it has more than 5 rows. if it is the case, I suggest the first 5. if it is not the case, I do the following chunks\n",
    "value = True\n",
    "key = sorted_common_df.shape[0]\n",
    "\n",
    "\n",
    "if sorted_common_df.shape[0] >= 5:\n",
    "    value = True\n",
    "    print(sorted_common_df['title'].head(5).to_list())\n",
    "else:\n",
    "    value = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) suggests other films\n",
    "# if there are less than 5 films, then suggest others in base of the user with the most similarity:\n",
    "\n",
    "\n",
    "#I create a new dataframe with the films of the two users, but without the one(s) in common\n",
    "final = 5\n",
    "sub_key = final - key\n",
    "\n",
    "common_titles = set(merged_users['title'].value_counts()[merged_users['title'].value_counts() > 1].index)\n",
    "\n",
    "#remove rows with common titles\n",
    "filtered_df = merged_users[merged_users['title'].isin(common_titles) == False]\n",
    "\n",
    "new_df = filtered_df.head(sub_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Icarus', 'Accidental Courtesy: Daryl Davis, Race & America', 'Heroin(e)', 'Killer Legends', 'Team Foxcatcher']\n"
     ]
    }
   ],
   "source": [
    "#first I have the common films, then I have the only-one-user films\n",
    "recomended_films = pd.concat([sorted_common_df, new_df])\n",
    "\n",
    "#since they are ordered, I just take the first 5 for the result\n",
    "#this is my final result.\n",
    "print(recomended_films['title'].head(5).to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Grouping Users together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will deal with clustering algorithms that will provide groups of Netflix users that are similar among them.\n",
    "\n",
    "To solve this task, you must accomplish the following stages:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Getting your data + feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)  Access to the data found in [this dataset](https://www.kaggle.com/datasets/vodclickstream/netflix-audience-behaviour-uk-movies)\n",
    "\n",
    "2)  Sometimes, the features (variables, fields) are not given in a dataset but can be created from it; this is known as *feature engineering*. For example, the original dataset has several clicks done by the same user, so grouping data by user_id will allow you to create new features **for each** user:\n",
    "\n",
    "    a)  Favorite genre (i.e., the genre on which the user spent the most time)\n",
    "\n",
    "    b)  Average click duration\n",
    "\n",
    "    c)  Time of the day (Morning/Afternoon/Night) when the user spends the most time on the platform (the time spent is tracked through the duration of the clicks)\n",
    "\n",
    "    d)  Is the user an old movie lover, or is he into more recent stuff (content released after 2010)?\n",
    "\n",
    "    e)  Average time spent a day by the user (considering only the days he logs in)\n",
    "\n",
    "So, in the end, you should have for each user_id five features.\n",
    "\n",
    "3)  Consider at least 10 additional features that can be generated for each user_id (you can use chatGPT or other LLM tools for suggesting features to create). Describe each of them and add them to the previous dataset you made (the one with five features). In the end, you should have for each user at least 15 features (5 recommended + 10 suggested by you)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Choose your features (variables)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that you have plenty of features to work with now. So, it would be best to find a way to reduce the dimensionality (reduce the number of variables to work with). You can follow the subsequent directions to achieve it:\n",
    "\n",
    "1)  *To normalise or not to normalise? That's the question*. Sometimes, it is worth normalizing (scaling) the features. Explain if it is a good idea to perform any normalization method. If you think the normalization should be used, apply it to your data (look at the available normalization functions in the `scikit-learn` library).\n",
    "\n",
    "2)  Select **one** method for dimensionality reduction and apply it to your data. Some suggestions are Principal Component Analysis, Multiple Correspondence Analysis, Singular Value Decomposition, Factor Analysis for Mixed Data, Two-Steps clustering. Make sure that the method you choose applies to the features you have or modify your data to be able to use it. Explain why you chose that method and the limitations it may have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Clustering!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)  Implement the K-means clustering algorithm (**not** ++: random initialization) using MapReduce. We ask you to write the algorithm from scratch following what you learned in class.\n",
    "\n",
    "2)  Find an optimal number of clusters. Use at least two different methods. If your algorithms provide diverse optimal K's, select one of them and explain why you chose it.\n",
    "\n",
    "3)  Run the algorithm on the data obtained from the dimensionality reduction.\n",
    "\n",
    "4)  Implement **K-means++** from scratch and explain the differences with the results you got earlier.\n",
    "\n",
    "5)  Ask ChatGPT to recommend other clustering algorithms and choose one. Explain your choice, then ask ChatGPT to implement it or use already implemented versions (e.g., the one provided in the scikit-learn library) and run it on your data. Explain the differences (if there are any) in the results. Which one is the best, in your opinion, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Analysing your results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are often encouraged to explain the main characteristics that your clusters have. The latter is called the *Characterizing Clusters* step. Thus, follow the next steps to do it:\n",
    "\n",
    "1)  Select 2-3 variables you think are relevant to identify the cluster of the customer. For example, Time_Day, Average Click Duration, etc.\n",
    "\n",
    "2)  Most of your selected variables will be numerical (continuous or discrete), then categorize them into four categories.\n",
    "\n",
    "3)  With the selected variables, perform pivot tables. On the horizontal axis, you will have the clusters, and on the vertical axis, you will have the categories of each variable. Notice that you have to do one pivot table per variable.\n",
    "\n",
    "4)  Calculate the percentage by column for each pivot table. The sum of each row (cluster) must be 100.\n",
    "\n",
    "5)  Interpret the results for each pivot table.\n",
    "\n",
    "6)  Use any known metrics to estimate clustering algorithm performance (how good are the clusters you found?). Comment on the results obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bonus Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remind you that we consider and grade the bonuses only if you complete the entire assignment.\n",
    "\n",
    "[Density-based clustering](https://wires.onlinelibrary.wiley.com/doi/epdf/10.1002/widm.30) identifies clusters as regions in the data space with high point density that are separated from other clusters by regions of low point density. The data points in the separating regions of low point density are typically considered noise or outliers. Typical algorithms that fall into this category are [OPTICS](https://dl.acm.org/doi/pdf/10.1145/304181.304187) and [DBSCAN](https://cdn.aaai.org/KDD/1996/KDD96-037.pdf).\n",
    "\n",
    "1)  Ask ChatGPT (or any other LLM tool) to list three algorithms for Density-Based Clustering. Choose one and use it on the same dataset you used in 2.3. Analyze your results: how different are they from the centroid-based version?\n",
    "\n",
    "__Note__: You can implement your algorithm from scratch or use the one implemented in the scikit-learn library; the choice is up to you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Command Line Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is another command line question to enjoy. We previously stated that using the command line tools is a skill that Data Scientists must master.\n",
    "\n",
    "In this question, you should use any command line tool that you know to answer the following questions using the same dataset that you have been using so far:\n",
    "  + What is the most-watched Netflix title?\n",
    "  + Report the average time between subsequent clicks on Netflix.com\n",
    "  + Provide the ID of the user that has spent the most time on Netflix\n",
    "    \n",
    "__Important note:__ You may work on this question in any environment (AWS, your PC command line, Jupyter notebook, etc.), but the final script must be placed in CommandLine.sh, which must be executable. Please run the script and include a __screenshot__ of the <ins>output</ins> in the notebook for evaluation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Algorithmic Question "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Federico studies in a demanding university where he has to take a certain number $N$ of exams to graduate,  but he is free to choose in which order he will take these exams. Federico is panicking since this university is not only one of the toughest in the world but also one of the weirdest. His final grade won't depend at all on the mark he gets in these courses: there's a precise evaluation system. \n",
    "\n",
    "He was given an initial personal score of $S$ when he enrolled, which changes every time he takes an exam: now comes the crazy part.\n",
    "He soon discovered that every of the $N$ exams he has to take is assigned a mark $p$. Once he has chosen an exam, his score becomes equal to the mark $p$, and at the same time, the scoring system changes:\n",
    "+ If he takes an \"easy\" exam (the score of the exam being less than his score), every other exam's mark is increased by the quantity $S - p$.\n",
    "+ If he takes a \"hard\" exam (the score of the exam is greater than his score), every other exam's mark is decreased by the quantity $p - S$.\n",
    "  \n",
    "So, for example, consider $S=8$ as the initial personal score. Federico must decide which exam he wants to take, being $[5,7,1]$ the marks list. If he takes the first one, being $5 < 8$ and $8 - 5 = 3$, the remaining list now becomes $[10,4]$, and his score is updated as $S = 5$.\n",
    "\n",
    "In this chaotic university where the only real exam seems to be choosing the best way to take exams, you are the poor student advisor who is facing a long queue of confused people who need some help. Federico is next in line,  and he comes up in turn with an inescapable question: he wants to know which is the highest score possible he could get. \n",
    "\n",
    "a) Fortunately, you have a computer app designed by a brilliant student. Federico wants you to show him the code which this app is based on\n",
    "because he wants to do paid counseling for other desperate students: in a *recursive* fashion, the helped helps the helpable.\n",
    "\n",
    "b) Federico is getting angry because he claims that your code is slow! Show him formally with a big-O notation that he is as crazy as this university! \n",
    "\n",
    "c) If, unfortunately, Federico is right in the grip of madness, he will threaten you to optimize the code through a different approach. \n",
    "You should end this theater of the absurd by any means! (And again, formally prove that you improved time complexity)\n",
    "\n",
    "d) Ask chatGPT for a third (optimized) implementation and analyze again its time complexity. Be careful (and crafty) in defining the prompt, and challenge the machine in this coding question!\n",
    "\n",
    "Here are some input/output examples (the first value is the initial personal score, and the second line contains the list of marks): \n",
    "\n",
    "__Input 1__\n",
    "```\n",
    "8\n",
    "5 7 1 \n",
    "```\n",
    "\n",
    "__Output 1__\n",
    "```\n",
    "11\n",
    "```\n",
    "\n",
    "__Input 2__\n",
    "```\n",
    "25\n",
    "18 24 21 32 27\n",
    "```\n",
    "\n",
    "__Output 2__\n",
    "```\n",
    "44\n",
    "```\n",
    "\n",
    "__Input 3__\n",
    "```\n",
    "30\n",
    "13 27 41 59 28 33 39 19 52 48 55 79\n",
    "```\n",
    "\n",
    "__Output 3__\n",
    "```\n",
    "109\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
