{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook by:\n",
    "\n",
    "* Lorenzo Pannacci 1948926\n",
    "* Stefano Rinaldi 1945551"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Startup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# LIBRARIES DOWNLOAD #\n",
    "######################\n",
    "\n",
    "install_packages = False\n",
    "if install_packages:\n",
    "    %pip install numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# LIBRARIES IMPORT #\n",
    "####################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Recommendation sytem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - top 10 films"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim is to find the top 10 film with most clicks for each user.\n",
    "\n",
    "<code style=\"background:red;color:black\"> this is what I am going to do step by step:</code>\n",
    "\n",
    "1) Load the data <br>\n",
    "2) Fix the dataset (if necessary) <br>\n",
    "3) Group by user and movie <br>\n",
    "4) Count the clicks <br>\n",
    "5) Sort the data by click count for each user.<br>\n",
    "6) Extract the top 10 movies for each user and take title and genre.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                           [None, None, None]\n",
       "1                     [None, None, None, None]\n",
       "2                                 [None, None]\n",
       "3                                 [None, None]\n",
       "4         [None, None, None, None, None, None]\n",
       "                          ...                 \n",
       "671731                                  [None]\n",
       "671732          [None, None, None, None, None]\n",
       "671733                      [None, None, None]\n",
       "671734                            [None, None]\n",
       "671735                            [None, None]\n",
       "Name: genres_list, Length: 671736, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1)/2) load the data and fix the dataset\n",
    "dataset = pd.read_csv('vodclickstream_uk_movies_03.csv')\n",
    "\n",
    "#clean it\n",
    "dataset = dataset.drop(\"Unnamed: 0\", axis=1)\n",
    "\n",
    "# Convert datetime column to a date\n",
    "dataset.datetime = pd.to_datetime(dataset.datetime)\n",
    "\n",
    "# Convert release_date column to a date\n",
    "dataset.release_date = pd.to_datetime(dataset.release_date, errors=\"coerce\")\n",
    "\n",
    "# Extract genres column to a new column genre_list that includes a list with all genres\n",
    "dataset[\"genres_list\"] = dataset.genres.apply(lambda row: [word.strip() for word in row.split(\",\")])\n",
    "\n",
    "#retrieve all unique film genres\n",
    "unique_genres = set()\n",
    "\n",
    "dataset[\"genres_list\"].apply(lambda row: [unique_genres.add(value) for value in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>title</th>\n",
       "      <th>click_count</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00004e2862</td>\n",
       "      <td>Hannibal</td>\n",
       "      <td>1</td>\n",
       "      <td>Crime, Drama, Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000052a0a0</td>\n",
       "      <td>Looper</td>\n",
       "      <td>9</td>\n",
       "      <td>Action, Drama, Sci-Fi, Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>000052a0a0</td>\n",
       "      <td>Frailty</td>\n",
       "      <td>3</td>\n",
       "      <td>Crime, Drama, Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>000052a0a0</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>3</td>\n",
       "      <td>Adventure, Comedy, Family, Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>000052a0a0</td>\n",
       "      <td>Resident Evil</td>\n",
       "      <td>2</td>\n",
       "      <td>Action, Horror, Sci-Fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613980</th>\n",
       "      <td>fffeac83be</td>\n",
       "      <td>To the Bone</td>\n",
       "      <td>1</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613981</th>\n",
       "      <td>fffeac83be</td>\n",
       "      <td>True Story</td>\n",
       "      <td>1</td>\n",
       "      <td>Crime, Drama, Mystery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613982</th>\n",
       "      <td>ffff2c5f9e</td>\n",
       "      <td>Forks Over Knives</td>\n",
       "      <td>1</td>\n",
       "      <td>Documentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613983</th>\n",
       "      <td>ffff2c5f9e</td>\n",
       "      <td>Hot Fuzz</td>\n",
       "      <td>1</td>\n",
       "      <td>Action, Comedy, Mystery, Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613984</th>\n",
       "      <td>ffffd36adf</td>\n",
       "      <td>Dead Man Walking</td>\n",
       "      <td>1</td>\n",
       "      <td>Crime, Drama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>447114 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           user_id              title  click_count  \\\n",
       "0       00004e2862           Hannibal            1   \n",
       "1       000052a0a0             Looper            9   \n",
       "10      000052a0a0            Frailty            3   \n",
       "13      000052a0a0            Jumanji            3   \n",
       "16      000052a0a0      Resident Evil            2   \n",
       "...            ...                ...          ...   \n",
       "613980  fffeac83be        To the Bone            1   \n",
       "613981  fffeac83be         True Story            1   \n",
       "613982  ffff2c5f9e  Forks Over Knives            1   \n",
       "613983  ffff2c5f9e           Hot Fuzz            1   \n",
       "613984  ffffd36adf   Dead Man Walking            1   \n",
       "\n",
       "                                    genres  \n",
       "0                   Crime, Drama, Thriller  \n",
       "1          Action, Drama, Sci-Fi, Thriller  \n",
       "10                  Crime, Drama, Thriller  \n",
       "13      Adventure, Comedy, Family, Fantasy  \n",
       "16                  Action, Horror, Sci-Fi  \n",
       "...                                    ...  \n",
       "613980                               Drama  \n",
       "613981               Crime, Drama, Mystery  \n",
       "613982                         Documentary  \n",
       "613983   Action, Comedy, Mystery, Thriller  \n",
       "613984                        Crime, Drama  \n",
       "\n",
       "[447114 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3)/4)/5)/6) group by user and movie, count clicks and sort them\n",
    "\n",
    "#create a new dataframe with the information I need\n",
    "users = dataset[[\"genres_list\", \"user_id\",\"title\", \"genres\"]]\n",
    "\n",
    "#group user and movie and count the click\n",
    "user_movie_counts =users.groupby(['user_id', 'title']).size().reset_index(name='click_count')\n",
    "\n",
    "#sort them\n",
    "user_movie_counts = user_movie_counts.sort_values(by=['user_id', 'click_count'], ascending=[True, False])\n",
    "\n",
    "#take the top 10 movies for each user\n",
    "user_movie_counts = user_movie_counts.groupby('user_id').head(10)\n",
    "\n",
    "#return a new dataframe with the top 10 movies for each user;\n",
    "# I give as information the user, the titple and the genre of the film (as well as the click count)\n",
    "#other pieces of information are not asked\n",
    "\n",
    "result = pd.merge(user_movie_counts, users[['user_id', 'title', 'genres']], on=['user_id', 'title'], how='left')\n",
    "\n",
    "result['genres'] = result['genres'].astype(str)\n",
    "\n",
    "#remove duplicates. I have to do this because the previous code gives the right answer, but\n",
    "#it creates n rows for the x value of the click count.\n",
    "#so if a film has 9 clicks, it creates 9 times that rows.\n",
    "result = result.drop_duplicates()\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>title</th>\n",
       "      <th>click_count</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70879</th>\n",
       "      <td>1df8e21154</td>\n",
       "      <td>Beastly</td>\n",
       "      <td>5</td>\n",
       "      <td>Drama, Fantasy, Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70884</th>\n",
       "      <td>1df8e21154</td>\n",
       "      <td>Titanic</td>\n",
       "      <td>4</td>\n",
       "      <td>Drama, Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70888</th>\n",
       "      <td>1df8e21154</td>\n",
       "      <td>Addicted</td>\n",
       "      <td>3</td>\n",
       "      <td>Drama, Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70891</th>\n",
       "      <td>1df8e21154</td>\n",
       "      <td>Match Point</td>\n",
       "      <td>3</td>\n",
       "      <td>Drama, Romance, Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70894</th>\n",
       "      <td>1df8e21154</td>\n",
       "      <td>My Last Day without You</td>\n",
       "      <td>3</td>\n",
       "      <td>Comedy, Drama, Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70897</th>\n",
       "      <td>1df8e21154</td>\n",
       "      <td>The Red Thread</td>\n",
       "      <td>3</td>\n",
       "      <td>Drama, Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70900</th>\n",
       "      <td>1df8e21154</td>\n",
       "      <td>After Earth</td>\n",
       "      <td>2</td>\n",
       "      <td>Action, Adventure, Sci-Fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70902</th>\n",
       "      <td>1df8e21154</td>\n",
       "      <td>Boomerang</td>\n",
       "      <td>2</td>\n",
       "      <td>Comedy, Drama, Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70904</th>\n",
       "      <td>1df8e21154</td>\n",
       "      <td>Diary of a Mad Black Woman</td>\n",
       "      <td>2</td>\n",
       "      <td>Comedy, Drama, Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70906</th>\n",
       "      <td>1df8e21154</td>\n",
       "      <td>Lift Me Up</td>\n",
       "      <td>2</td>\n",
       "      <td>Family</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          user_id                       title  click_count  \\\n",
       "70879  1df8e21154                     Beastly            5   \n",
       "70884  1df8e21154                     Titanic            4   \n",
       "70888  1df8e21154                    Addicted            3   \n",
       "70891  1df8e21154                 Match Point            3   \n",
       "70894  1df8e21154     My Last Day without You            3   \n",
       "70897  1df8e21154              The Red Thread            3   \n",
       "70900  1df8e21154                 After Earth            2   \n",
       "70902  1df8e21154                   Boomerang            2   \n",
       "70904  1df8e21154  Diary of a Mad Black Woman            2   \n",
       "70906  1df8e21154                  Lift Me Up            2   \n",
       "\n",
       "                          genres  \n",
       "70879    Drama, Fantasy, Romance  \n",
       "70884             Drama, Romance  \n",
       "70888            Drama, Thriller  \n",
       "70891   Drama, Romance, Thriller  \n",
       "70894     Comedy, Drama, Romance  \n",
       "70897             Drama, Romance  \n",
       "70900  Action, Adventure, Sci-Fi  \n",
       "70902     Comedy, Drama, Romance  \n",
       "70904     Comedy, Drama, Romance  \n",
       "70906                     Family  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#here you can retrieve the result for a specific user\n",
    "### CHANGE THE USER HERE TO THE DESIRED ONE\n",
    "input_user = \"1df8e21154\"\n",
    "\n",
    "specific_user = result[result['user_id'] == input_user]\n",
    "\n",
    "specific_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"background:orange;color:black\">**Little comment**</code> <br>\n",
    "here we can see the result for a specific user (and before also for all users together). Result is ordered by click count. In this case, we are taking into consideration all clicks and films and users. Even if a click has a duration of 0, it is taken into consideration as the user showed interest to the film and so is useful for both building the next hashing function and for counting the film with most clicks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Minhash Signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I try to group users together into buckets based on the film genres. <br>\n",
    "<code style=\"background:red;color:black\"> this is what I am going to do step by step:</code>\n",
    "\n",
    "0) **group users and genres**:<br>\n",
    "I create a dictionary in which the keys are the *user_id* and the values the *genres* associated to each user. <br>\n",
    "1) **create a matrix with genres for each user** <br>\n",
    "I create a matrix in which I assign 1 if the user has seen that genre, 0 if the user has not\n",
    "2) **compute signatures for each user** <br>\n",
    "given the matrix, I first create some random hash functions and then I compute the signature for each user.\n",
    "3) **bucketing**<br>\n",
    "Now that I have the signatures, I divide users into buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0)\n",
    "### Here I create a 2 columns dataframe. one with the users_id and one with the genres. \n",
    "#each row has only one genre. this will be needed for creating a dictionary with users as keys and genres as values\n",
    "\n",
    "small_dataset = dataset[[\"genres\",\"user_id\"]]\n",
    "\n",
    "exploded_genres = small_dataset.assign(genres=dataset['genres'].str.split(',')).explode('genres').reset_index(drop=True)\n",
    "\n",
    "exploded_genres['genres'] = exploded_genres['genres'].str.strip()\n",
    "\n",
    "# Duplicate the 'user_id' column to match the number of rows in the exploded DataFrame\n",
    "genres_with_user = pd.concat([exploded_genres['user_id'], exploded_genres['genres']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) create a dictionary where I have as keys the user_id and as values all genres related to it\n",
    "\n",
    "user_genres_dict = {}\n",
    "\n",
    "#iterare over dataframe\n",
    "for index, row in genres_with_user.iterrows():\n",
    "    user_id = row['user_id']\n",
    "    genre = row['genres']\n",
    "\n",
    "    #if user_id already exist, append \n",
    "    if user_id in user_genres_dict:\n",
    "        user_genres_dict[user_id].append(genre)\n",
    "    #if user_id does not exist, create the key\n",
    "    else:\n",
    "        user_genres_dict[user_id] = [genre]\n",
    "\n",
    "\n",
    "#then I convert into a dataframe (it will be necessary later)\n",
    "user_genres_df = pd.DataFrame(list(user_genres_dict.items()), columns=['user_id', 'genres'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Action</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Biography</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>Crime</th>\n",
       "      <th>Documentary</th>\n",
       "      <th>Drama</th>\n",
       "      <th>Family</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>...</th>\n",
       "      <th>News</th>\n",
       "      <th>Reality-TV</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Sci-Fi</th>\n",
       "      <th>Short</th>\n",
       "      <th>Sport</th>\n",
       "      <th>Talk-Show</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>War</th>\n",
       "      <th>Western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1dea19f6fe</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544dcbc510</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7cbcc791bf</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ebf43c36b6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a57c992287</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Action  Adventure  Animation  Biography  Comedy  Crime  \\\n",
       "1dea19f6fe       0          0          0          0       1      0   \n",
       "544dcbc510       0          1          0          0       1      0   \n",
       "7cbcc791bf       1          1          1          0       1      1   \n",
       "ebf43c36b6       1          1          1          1       1      1   \n",
       "a57c992287       1          1          1          1       1      1   \n",
       "\n",
       "            Documentary  Drama  Family  Fantasy  ...  News  Reality-TV  \\\n",
       "1dea19f6fe            0      1       0        0  ...     0           0   \n",
       "544dcbc510            0      1       0        1  ...     0           0   \n",
       "7cbcc791bf            0      0       1        1  ...     0           0   \n",
       "ebf43c36b6            0      1       1        1  ...     0           0   \n",
       "a57c992287            1      1       1        1  ...     0           0   \n",
       "\n",
       "            Romance  Sci-Fi  Short  Sport  Talk-Show  Thriller  War  Western  \n",
       "1dea19f6fe        1       0      0      0          0         0    0        0  \n",
       "544dcbc510        1       0      0      0          0         1    0        0  \n",
       "7cbcc791bf        1       0      0      0          0         1    0        0  \n",
       "ebf43c36b6        0       1      0      0          0         1    0        0  \n",
       "a57c992287        0       1      1      1          0         1    1        1  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1) create the matrix \n",
    "\n",
    "#take unique genres of films\n",
    "#unique_genres was already created before! I am just recalling it for continuity\n",
    "unique_genres = sorted(list(unique_genres))\n",
    "\n",
    "#create my matrix. columns are genres and the index is user_id\n",
    "my_matrix = pd.DataFrame(0, index=dataset[\"user_id\"].unique(), columns=unique_genres)\n",
    "#put 1 if the user has that genre, put 0 if there is not\n",
    "for user_id, genres in user_genres_dict.items():\n",
    "    for genre in genres:\n",
    "        if genre in unique_genres:\n",
    "            my_matrix.loc[user_id, genre] = 1\n",
    "\n",
    "my_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2) compute signature for all users\n",
    "\n",
    "#I create my hash function.\n",
    "#I give a number of my choice and then that number of functions are created. the coefficient x and y are changed every time.\n",
    "#basically in the next function I have a fixed equation but every time the parameters (x,y) are changed randomly\n",
    "def generate_hash(n_hashes):\n",
    "    hash_functions = []\n",
    "\n",
    "    for k in range(n_hashes):\n",
    "        x = random.randint(1, 100)\n",
    "        y = random.randint(0, 100)\n",
    "        hash_functions.append((x, y))\n",
    "\n",
    "    return hash_functions\n",
    "\n",
    "#calculate signature \n",
    "def calculate_signature(user_preferences, n_hashes, n_genres, list_functions):\n",
    "    signature = [float('inf')] * n_hashes\n",
    "\n",
    "    #for every element of my matrix (which is actually a dataframe)\n",
    "    #for every genre index and the binary encoding (0, 1)\n",
    "    for index, present in enumerate(user_preferences):\n",
    "        #for each genre do the hash functions\n",
    "        for k, hash_function in enumerate(list_functions):\n",
    "            x, y = hash_function\n",
    "            hash_value = (x * index * present + y) % n_genres\n",
    "            #retain only the minimun\n",
    "            signature[k] = min(signature[k], hash_value)\n",
    "    #so here I will have the minimum \n",
    "    return signature\n",
    "\n",
    "#my variables\n",
    "#the more the n_hashes, the better but it will require more time\n",
    "genres = list(unique_genres)\n",
    "n_hashes = 15\n",
    "n_genres = len(genres)\n",
    "list_functions = generate_hash(n_hashes)\n",
    "\n",
    "#dictionary that will store the signatures for each user\n",
    "signatures = {}\n",
    "\n",
    "#I calculare the signature for each user in the matrix (which is a dataframe actually)\n",
    "for user_id, user_preferences in my_matrix.iterrows():\n",
    "    signature = calculate_signature(user_preferences, n_hashes, n_genres, list_functions)\n",
    "    signatures[user_id] = signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3) divide users into buckets basing on their signature \n",
    "\n",
    "def bucketing(signatures, n_hashes, n_bands):\n",
    "    #number of rows per band\n",
    "    n_rows = n_hashes // n_bands\n",
    "\n",
    "    #dictionary that will store the buckets\n",
    "    buckets = defaultdict(list)\n",
    "\n",
    "    #for each user:\n",
    "    for user_id, signature in signatures.items():\n",
    "        for band_index in range(n_bands):\n",
    "            #calculates the start and end indices for the current band.\n",
    "            start = band_index * n_rows\n",
    "            end = (band_index + 1) * n_rows\n",
    "\n",
    "            #extract the band from the minhash signature and convert it to a tuple\n",
    "            band = tuple(signature[start:end])\n",
    "\n",
    "            #append the user to the corresponding bucket based on the value.\n",
    "            buckets[band].append(user_id)\n",
    "\n",
    "    return buckets\n",
    "\n",
    "n_bands= 6\n",
    "buckets = bucketing(signatures, n_hashes, n_bands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Locality-Sensitive Hashing (LSH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"background:red;color:black\"> this is what I am going to do step by step:</code> <br>\n",
    "\n",
    "1) **input the user** <br>\n",
    "input the user to which you wish to suggest 5 films. <br>\n",
    "2) **Identify the similar users** <br>\n",
    "Given my user, I want to find all other users that share with my user the same bucket(s) as they must share a common sense of genre film. For the users I find, I count in how many buckets they appear (in the sense that I count how many bucket they share with my user). The more buckets they share, the more probability they are very similar.<br>\n",
    "3) **select the users** <br>\n",
    "From the similar users I just found (from the previous step, so the users that share the most buckets with my user) now I compute similarity and thake the top 2 users. They will be used for suggesting films.\n",
    "5) **suggest common films** <br>\n",
    "If these two users have any movies in common, recommend those movies in order based on the total number of clicks by these users. <br>\n",
    "6) **suggest other films** <br>\n",
    "If there are no common movies or less than 5 common movies, try to propose the most clicked movies by the most similar user first, followed by the other user. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1)\n",
    "#INPUT HERE THE USER TO WHICH YOU WANT TO FIND THE MOST SIMILAR USERS (and therefore suggest to them 5 films)\n",
    "user_id_input = \"20c7acabb0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2) fund the similar users \n",
    "\n",
    "#The idea is that I give a user, and it count occurrences in the same buckets of the other users.\n",
    "#If my input user and the other users appears into a lot of common buckets, it is very likely that they have same interests \n",
    "#The output is a list sorted in descending order of occurrences.\n",
    "\n",
    "def similar_users(user_id, n_users=10):\n",
    "    #make a list with all users into the buckets in which the input user_id appears\n",
    "    user_buckets = [bucket for bucket in buckets.values() if user_id in bucket]\n",
    "    all_users = [user for bucket in user_buckets for user in bucket if user != user_id]\n",
    "\n",
    "    #count occurrences of users \n",
    "    user_occurrences = Counter(all_users)\n",
    "\n",
    "    #sort \n",
    "    sorted_user_occurrences = user_occurrences.most_common(n_users)\n",
    "    return sorted_user_occurrences\n",
    "\n",
    "most_similar_users = similar_users(user_id_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      user_id  similarity\n",
      "6  dd77cda733    0.642857\n",
      "5  9c1ab7b359    0.636364\n",
      "2  775a8ab995    0.600000\n",
      "7  7ddbaf4702    0.545455\n",
      "8  1eb52c1c77    0.500000\n",
      "4  60455cc93f    0.411765\n",
      "1  3a142199a8    0.400000\n",
      "0  25a4de9d06    0.333333\n",
      "3  f9a35f2935    0.294118\n",
      "9  68164a256f    0.142857\n"
     ]
    }
   ],
   "source": [
    "#3) select users by similarity \n",
    "\n",
    "#given the 10 users I received from the previous filtering, \n",
    "#I compute on them jaccard similarity and sort them, so that I am 100% sure that they are ordered into the right way.\n",
    "#then, I take the 2 users with the highest score\n",
    "\n",
    "def calculate_jaccard_similarity(user1_genres, user2_genres):\n",
    "    #jaccard similarity\n",
    "    intersection = len(set(user1_genres) & set(user2_genres))\n",
    "    union = len(set(user1_genres).union(user2_genres))\n",
    "    return intersection / union\n",
    "\n",
    "input_user_genres = user_genres_df[user_genres_df['user_id'] == user_id_input]['genres'].iloc[0]\n",
    "\n",
    "#store the similarity result\n",
    "similarities = []\n",
    "\n",
    "for similar_user_id, _ in most_similar_users:\n",
    "    similar_user_genres = user_genres_df[user_genres_df['user_id'] == similar_user_id]['genres'].iloc[0]\n",
    "    \n",
    "    jaccard_similarity = calculate_jaccard_similarity(input_user_genres, similar_user_genres)\n",
    "    \n",
    "    similarities.append({'user_id': similar_user_id, 'similarity': jaccard_similarity})\n",
    "\n",
    "similarities_df = pd.DataFrame(similarities)\n",
    "similarities_df = similarities_df.sort_values(\"similarity\", ascending= False)\n",
    "print(similarities_df)\n",
    "\n",
    "#take the top 2 users\n",
    "users = similarities_df.user_id.head(2)\n",
    "\n",
    "user_1 =  users.iloc[0]\n",
    "user_2 =  users.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>sum_click_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>432380</th>\n",
       "      <td>The Big Short</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                title  sum_click_count\n",
       "432380  The Big Short                2"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4) If these two users have any movies in common, recommend those movies based on the total number of clicks by these users\n",
    "#extract common movies\n",
    "\n",
    "# Filter data for each user\n",
    "user1_movies = user_movie_counts[user_movie_counts['user_id'] == user_1]\n",
    "user2_movies = user_movie_counts[user_movie_counts['user_id'] == user_2]\n",
    "\n",
    "#merge the dataframes such that I have all the movies together\n",
    "merged_users = pd.concat([user1_movies, user2_movies])\n",
    "\n",
    "#NOW, I suggest all movies that are in common and I order them by clicks count.\n",
    "\n",
    "user_groups = merged_users.groupby('user_id')['title'].apply(set)\n",
    "\n",
    "# Find the common titles between the two users\n",
    "common_titles = set.intersection(user_groups.iloc[0], user_groups.iloc[1])\n",
    "\n",
    "common_df = merged_users[merged_users['title'].isin(common_titles)].copy()\n",
    "\n",
    "#count clicks for each film\n",
    "common_df['sum_click_count'] = common_df.groupby('title')['click_count'].transform('sum')\n",
    "\n",
    "#sort by the sum of cliks\n",
    "sorted_common_df = common_df.sort_values(by='sum_click_count', ascending=False)\n",
    "\n",
    "#films in common between the two users, drop dublicates\n",
    "sorted_common_df = sorted_common_df.drop_duplicates(subset='title')\n",
    "sorted_common_df = sorted_common_df[[\"title\",\"sum_click_count\"]]\n",
    "sorted_common_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "### here I do the following:\n",
    "#If the common films are 5, we are done.\n",
    "#if they are less than 5, I suggest the remaining ones from the user by greatest similarity.\n",
    "#if the user with greatest similarity has no more other films, I will pass to the second user.\n",
    "#ex: if the common films are 3, I will suggest only 2 films from the user with greatest similarity / the second user.\n",
    "\n",
    "#I check if it has more than 5 rows. if it is the case, I suggest the first 5. if it is not the case, I do the following chunks\n",
    "value = True\n",
    "key = sorted_common_df.shape[0]\n",
    "\n",
    "\n",
    "if sorted_common_df.shape[0] >= 5:\n",
    "    value = True\n",
    "    print(sorted_common_df['title'].head(5).to_list())\n",
    "else:\n",
    "    value = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) suggests other films\n",
    "# if there are less than 5 films, then suggest others in base of the user with the most similarity:\n",
    "\n",
    "\n",
    "#I create a new dataframe with the films of the two users, but without the one(s) in common\n",
    "final = 5\n",
    "sub_key = final - key\n",
    "\n",
    "common_titles = set(merged_users['title'].value_counts()[merged_users['title'].value_counts() > 1].index)\n",
    "\n",
    "#remove rows with common titles\n",
    "filtered_df = merged_users[merged_users['title'].isin(common_titles) == False]\n",
    "\n",
    "new_df = filtered_df.head(sub_key)\n",
    "\n",
    "#first I have the common films, then I have the only-one-user films\n",
    "recomended_films = pd.concat([sorted_common_df, new_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Big Short', 'Eddie the Eagle', 'London Has Fallen', 'Minimalism: A Documentary About the Important Things', 'Love']\n"
     ]
    }
   ],
   "source": [
    "#since they are ordered, I just take the first 5 for the result\n",
    "#this is my final result.\n",
    "print(recomended_films['title'].head(5).to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"background:orange;color:black\">**little comment**</code>:\n",
    "\n",
    "As we can see, the function works correctly. our input user *20c7acabb0* has seen Action, Drama, Sport, Music, War, Thriller, Sci-Fi and History exactly as *dd77cda733* and *9c1ab7b359* with few differences in few genres that one has seen and the other has not, meaning that the function is working correctly. <br>\n",
    "Nevertheless, there are some point that needs to be made. In this way, we are suggesting the films that may be liked by a user, but this type of LSH is not taking into considerations repetitions in the sense that if a user sees 11 film, 10 of which are Comedy and 1 Drama, it counts Comedy and Drama as equals as it will insert 1 in the matrix, whereas the user has a clearly interest into the comedy genre. In order to build a better experience for the user, the function should take into account that and maybe LSH is not the right function for performing that. Nevertheless, if we implement a function as I am suggesting, it will give priority to users who whatches a lot of films making it messier for users who whatches just a couple of films.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Grouping Users together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will deal with clustering algorithms that will provide groups of Netflix users that are similar among them.\n",
    "\n",
    "To solve this task, you must accomplish the following stages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Getting your data + feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)  Access to the data found in [this dataset](https://www.kaggle.com/datasets/vodclickstream/netflix-audience-behaviour-uk-movies)\n",
    "\n",
    "2)  Sometimes, the features (variables, fields) are not given in a dataset but can be created from it; this is known as *feature engineering*. For example, the original dataset has several clicks done by the same user, so grouping data by user_id will allow you to create new features **for each** user:\n",
    "\n",
    "    a)  Favorite genre (i.e., the genre on which the user spent the most time)\n",
    "\n",
    "    b)  Average click duration\n",
    "\n",
    "    c)  Time of the day (Morning/Afternoon/Night) when the user spends the most time on the platform (the time spent is tracked through the duration of the clicks)\n",
    "\n",
    "    d)  Is the user an old movie lover, or is he into more recent stuff (content released after 2010)?\n",
    "\n",
    "    e)  Average time spent a day by the user (considering only the days he logs in)\n",
    "\n",
    "So, in the end, you should have for each user_id five features.\n",
    "\n",
    "3)  Consider at least 10 additional features that can be generated for each user_id (you can use chatGPT or other LLM tools for suggesting features to create). Describe each of them and add them to the previous dataset you made (the one with five features). In the end, you should have for each user at least 15 features (5 recommended + 10 suggested by you)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Choose your features (variables)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that you have plenty of features to work with now. So, it would be best to find a way to reduce the dimensionality (reduce the number of variables to work with). You can follow the subsequent directions to achieve it:\n",
    "\n",
    "1)  *To normalise or not to normalise? That's the question*. Sometimes, it is worth normalizing (scaling) the features. Explain if it is a good idea to perform any normalization method. If you think the normalization should be used, apply it to your data (look at the available normalization functions in the `scikit-learn` library).\n",
    "\n",
    "2)  Select **one** method for dimensionality reduction and apply it to your data. Some suggestions are Principal Component Analysis, Multiple Correspondence Analysis, Singular Value Decomposition, Factor Analysis for Mixed Data, Two-Steps clustering. Make sure that the method you choose applies to the features you have or modify your data to be able to use it. Explain why you chose that method and the limitations it may have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Clustering!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)  Implement the K-means clustering algorithm (**not** ++: random initialization) using MapReduce. We ask you to write the algorithm from scratch following what you learned in class.\n",
    "\n",
    "2)  Find an optimal number of clusters. Use at least two different methods. If your algorithms provide diverse optimal K's, select one of them and explain why you chose it.\n",
    "\n",
    "3)  Run the algorithm on the data obtained from the dimensionality reduction.\n",
    "\n",
    "4)  Implement **K-means++** from scratch and explain the differences with the results you got earlier.\n",
    "\n",
    "5)  Ask ChatGPT to recommend other clustering algorithms and choose one. Explain your choice, then ask ChatGPT to implement it or use already implemented versions (e.g., the one provided in the scikit-learn library) and run it on your data. Explain the differences (if there are any) in the results. Which one is the best, in your opinion, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Analysing your results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are often encouraged to explain the main characteristics that your clusters have. The latter is called the *Characterizing Clusters* step. Thus, follow the next steps to do it:\n",
    "\n",
    "1)  Select 2-3 variables you think are relevant to identify the cluster of the customer. For example, Time_Day, Average Click Duration, etc.\n",
    "\n",
    "2)  Most of your selected variables will be numerical (continuous or discrete), then categorize them into four categories.\n",
    "\n",
    "3)  With the selected variables, perform pivot tables. On the horizontal axis, you will have the clusters, and on the vertical axis, you will have the categories of each variable. Notice that you have to do one pivot table per variable.\n",
    "\n",
    "4)  Calculate the percentage by column for each pivot table. The sum of each row (cluster) must be 100.\n",
    "\n",
    "5)  Interpret the results for each pivot table.\n",
    "\n",
    "6)  Use any known metrics to estimate clustering algorithm performance (how good are the clusters you found?). Comment on the results obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bonus Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remind you that we consider and grade the bonuses only if you complete the entire assignment.\n",
    "\n",
    "[Density-based clustering](https://wires.onlinelibrary.wiley.com/doi/epdf/10.1002/widm.30) identifies clusters as regions in the data space with high point density that are separated from other clusters by regions of low point density. The data points in the separating regions of low point density are typically considered noise or outliers. Typical algorithms that fall into this category are [OPTICS](https://dl.acm.org/doi/pdf/10.1145/304181.304187) and [DBSCAN](https://cdn.aaai.org/KDD/1996/KDD96-037.pdf).\n",
    "\n",
    "1)  Ask ChatGPT (or any other LLM tool) to list three algorithms for Density-Based Clustering. Choose one and use it on the same dataset you used in 2.3. Analyze your results: how different are they from the centroid-based version?\n",
    "\n",
    "__Note__: You can implement your algorithm from scratch or use the one implemented in the scikit-learn library; the choice is up to you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ask ChatGPT (or any other LLM tool) to list three algorithms for Density-Based Clustering:** <br>\n",
    "1) DBSCAN (Density-Based Spatial Clustering of Applications with Noise)<br>\n",
    "2) OPTICS (Ordering Points To Identify the Clustering Structure) <br>\n",
    "3) HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here what I am going to do: <br>\n",
    "1) load the dataset of 2.3 <br>\n",
    "2) apply the dataset to the chosen algorithm (OPTICS) <br>\n",
    "3) compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Command Line Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is another command line question to enjoy. We previously stated that using the command line tools is a skill that Data Scientists must master.\n",
    "\n",
    "In this question, you should use any command line tool that you know to answer the following questions using the same dataset that you have been using so far:\n",
    "  + What is the most-watched Netflix title?\n",
    "  + Report the average time between subsequent clicks on Netflix.com\n",
    "  + Provide the ID of the user that has spent the most time on Netflix\n",
    "    \n",
    "__Important note:__ You may work on this question in any environment (AWS, your PC command line, Jupyter notebook, etc.), but the final script must be placed in CommandLine.sh, which must be executable. Please run the script and include a __screenshot__ of the <ins>output</ins> in the notebook for evaluation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Algorithmic Question "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Federico studies in a demanding university where he has to take a certain number $N$ of exams to graduate,  but he is free to choose in which order he will take these exams. Federico is panicking since this university is not only one of the toughest in the world but also one of the weirdest. His final grade won't depend at all on the mark he gets in these courses: there's a precise evaluation system. \n",
    "\n",
    "He was given an initial personal score of $S$ when he enrolled, which changes every time he takes an exam: now comes the crazy part.\n",
    "He soon discovered that every of the $N$ exams he has to take is assigned a mark $p$. Once he has chosen an exam, his score becomes equal to the mark $p$, and at the same time, the scoring system changes:\n",
    "+ If he takes an \"easy\" exam (the score of the exam being less than his score), every other exam's mark is increased by the quantity $S - p$.\n",
    "+ If he takes a \"hard\" exam (the score of the exam is greater than his score), every other exam's mark is decreased by the quantity $p - S$.\n",
    "  \n",
    "So, for example, consider $S=8$ as the initial personal score. Federico must decide which exam he wants to take, being $[5,7,1]$ the marks list. If he takes the first one, being $5 < 8$ and $8 - 5 = 3$, the remaining list now becomes $[10,4]$, and his score is updated as $S = 5$.\n",
    "\n",
    "In this chaotic university where the only real exam seems to be choosing the best way to take exams, you are the poor student advisor who is facing a long queue of confused people who need some help. Federico is next in line,  and he comes up in turn with an inescapable question: he wants to know which is the highest score possible he could get. \n",
    "\n",
    "a) Fortunately, you have a computer app designed by a brilliant student. Federico wants you to show him the code which this app is based on\n",
    "because he wants to do paid counseling for other desperate students: in a *recursive* fashion, the helped helps the helpable.\n",
    "\n",
    "b) Federico is getting angry because he claims that your code is slow! Show him formally with a big-O notation that he is as crazy as this university! \n",
    "\n",
    "c) If, unfortunately, Federico is right in the grip of madness, he will threaten you to optimize the code through a different approach. \n",
    "You should end this theater of the absurd by any means! (And again, formally prove that you improved time complexity)\n",
    "\n",
    "d) Ask chatGPT for a third (optimized) implementation and analyze again its time complexity. Be careful (and crafty) in defining the prompt, and challenge the machine in this coding question!\n",
    "\n",
    "Here are some input/output examples (the first value is the initial personal score, and the second line contains the list of marks): \n",
    "\n",
    "__Input 1__\n",
    "```\n",
    "8\n",
    "5 7 1 \n",
    "```\n",
    "\n",
    "__Output 1__\n",
    "```\n",
    "11\n",
    "```\n",
    "\n",
    "__Input 2__\n",
    "```\n",
    "25\n",
    "18 24 21 32 27\n",
    "```\n",
    "\n",
    "__Output 2__\n",
    "```\n",
    "44\n",
    "```\n",
    "\n",
    "__Input 3__\n",
    "```\n",
    "30\n",
    "13 27 41 59 28 33 39 19 52 48 55 79\n",
    "```\n",
    "\n",
    "__Output 3__\n",
    "```\n",
    "109\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
