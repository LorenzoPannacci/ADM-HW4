{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook by:\n",
    "\n",
    "* Lorenzo Pannacci 1948926\n",
    "* Stefano Rinaldi 1945551"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Startup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# LIBRARIES DOWNLOAD #\n",
    "######################\n",
    "\n",
    "install_packages = True\n",
    "if install_packages:\n",
    "    %pip install numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# LIBRARIES IMPORT #\n",
    "####################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Recommendation sytem \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - top 10 films\n",
    "\n",
    "\n",
    "for this exercise I did not understand well if I have to consider only the clicks or also the generes of a film. Either way, I will propose both solutions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### solution 1\n",
    "**taking into considetation only click counts**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what I am going to do steps to do: \n",
    "\n",
    "1) Load the data <br>\n",
    "2) Fix the dataset <br>\n",
    "3) Group by user and movie <br>\n",
    "4) Count the clicks <br>\n",
    "5) Sort the data by click count for each user.<br>\n",
    "6) Extract the top 10 movies for each user.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1)/2) load the data and fix the dataset\n",
    "dataset = pd.read_csv('vodclickstream_uk_movies_03.csv')\n",
    "\n",
    "#clean it\n",
    "dataset = dataset.drop(\"Unnamed: 0\", axis=1)\n",
    "#Convert genres from comma separated value to a list of values\n",
    "dataset[\"genres_list\"] = dataset.genres.apply(lambda row: row.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.datetime = pd.to_datetime(dataset.datetime, errors='coerce')\n",
    "#dataset.release_date = pd.to_datetime(dataset.release_date, errors='coerce')\n",
    "\n",
    "#identify unique values:\n",
    "#unique_genres = set()\n",
    "#dataset['genres_list'].apply(lambda row: [unique_genres.add(value) for value in row])\n",
    "#unique_genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3)/4)/5)/6) group by user and movie, count clicks and sort them\n",
    "\n",
    "#create a new dataframe with the information I need\n",
    "users = dataset[[\"genres_list\", \"user_id\",\"title\", \"genres\"]]\n",
    "\n",
    "#group user and movie and count the click\n",
    "user_movie_counts =users.groupby(['user_id', 'title']).size().reset_index(name='click_count')\n",
    "\n",
    "#sort them\n",
    "user_movie_counts = user_movie_counts.sort_values(by=['user_id', 'click_count'], ascending=[True, False])\n",
    "\n",
    "#take the top 10 movies for each user\n",
    "user_movie_counts = user_movie_counts.groupby('user_id').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           user_id              title  click_count  \\\n",
      "0       00004e2862           Hannibal            1   \n",
      "1       000052a0a0             Looper            9   \n",
      "10      000052a0a0            Frailty            3   \n",
      "13      000052a0a0            Jumanji            3   \n",
      "16      000052a0a0      Resident Evil            2   \n",
      "...            ...                ...          ...   \n",
      "613980  fffeac83be        To the Bone            1   \n",
      "613981  fffeac83be         True Story            1   \n",
      "613982  ffff2c5f9e  Forks Over Knives            1   \n",
      "613983  ffff2c5f9e           Hot Fuzz            1   \n",
      "613984  ffffd36adf   Dead Man Walking            1   \n",
      "\n",
      "                                    genres  \n",
      "0                   Crime, Drama, Thriller  \n",
      "1          Action, Drama, Sci-Fi, Thriller  \n",
      "10                  Crime, Drama, Thriller  \n",
      "13      Adventure, Comedy, Family, Fantasy  \n",
      "16                  Action, Horror, Sci-Fi  \n",
      "...                                    ...  \n",
      "613980                               Drama  \n",
      "613981               Crime, Drama, Mystery  \n",
      "613982                         Documentary  \n",
      "613983   Action, Comedy, Mystery, Thriller  \n",
      "613984                        Crime, Drama  \n",
      "\n",
      "[447114 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "#return a new dataframe with the top 10 movies for each user;\n",
    "# I give as information the user, the titple and the genre of the film (as well as the click count)\n",
    "#other pieces of information are not asked\n",
    "\n",
    "result = pd.merge(user_movie_counts, users[['user_id', 'title', 'genres']], on=['user_id', 'title'], how='left')\n",
    "\n",
    "result['genres'] = result['genres'].astype(str)\n",
    "\n",
    "#remove duplicates. I have to do this because the previous code gives the right answer, but\n",
    "#it creates n rows for the x value of the click count.\n",
    "#so if a film has 9 clicks, it creates 9 times that rows.\n",
    "result = result.drop_duplicates()\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### solution 2\n",
    "**take into consideration both clicks and genres of a film**\n",
    "\n",
    "In this case, I will extract the top 10 films by clicks from user, but then I will sort those films not only on the bases of the clicks but also on the genres it has, taking into considerations how many times a user saw a particular genre (still in the temporary top 10 films), giving to it priority. <br>\n",
    "The first filter will be the click-counts but then I will apply also a \"genre\" filter. This is also done because a lot of films have the same amount of clicks, so how can I order them giving them priority? I do this by using the genre of the film. To sum up, if a genre (such as \"Drama\") has been clicked by the user 10 times and another one (\"Comedy\"), has been clicked 2 times, two films with the same amounts of clicks, will be ordered by the score of their genre, such that \"Drama\" in this case will have priority. <br>\n",
    "\n",
    "steps to folow for this scenario: <br>\n",
    "\n",
    "7) divide the \"genre\" column <br>\n",
    "8) count how many times a genre appear for each user <br>\n",
    "9) merge the result with the previous dataset <br>\n",
    "10) sum the clicks and the genre to have the score of the film <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           user_id    genres  genre_count\n",
      "0       68768e88bf  Thriller           10\n",
      "1       b35d4891af  Thriller           10\n",
      "2       4b44d7203d    Action           10\n",
      "3       60716debd8    Comedy           10\n",
      "4       ecab823e36    Comedy           10\n",
      "...            ...       ...          ...\n",
      "847196  64a9e13237  Thriller            1\n",
      "847197  64aa0228d8     Drama            1\n",
      "847198  64aa0228d8   Romance            1\n",
      "847199  64aa4cfed1    Action            1\n",
      "847200  ffffd36adf     Drama            1\n",
      "\n",
      "[847201 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "####the code above is still the same, you have to add only these chunks here:\n",
    "# 7)/8) extract the genres from the column, specifically for those films with more than one genre. count the occurrences\n",
    "\n",
    "#take genres\n",
    "flat_genres = result['genres'].explode().str.split(',').explode().str.strip()\n",
    "\n",
    "#merge with the user_id \n",
    "flat_genres_with_user = pd.concat([result['user_id'], flat_genres], axis=1)\n",
    "\n",
    "#count the occurrences of each genre for each user\n",
    "genre_counts = flat_genres_with_user.value_counts().reset_index(name='genre_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           user_id              title  click_count  \\\n",
      "0       00004e2862           Hannibal            1   \n",
      "1       000052a0a0             Looper            9   \n",
      "2       000052a0a0            Frailty            3   \n",
      "3       000052a0a0            Jumanji            3   \n",
      "4       000052a0a0      Resident Evil            2   \n",
      "...            ...                ...          ...   \n",
      "447109  fffeac83be        To the Bone            1   \n",
      "447110  fffeac83be         True Story            1   \n",
      "447111  ffff2c5f9e  Forks Over Knives            1   \n",
      "447112  ffff2c5f9e           Hot Fuzz            1   \n",
      "447113  ffffd36adf   Dead Man Walking            1   \n",
      "\n",
      "                                      genres  genre_count  \n",
      "0                   [Crime, Drama, Thriller]            3  \n",
      "1          [Action, Drama, Sci-Fi, Thriller]           18  \n",
      "2                   [Crime, Drama, Thriller]           11  \n",
      "3       [Adventure, Comedy, Family, Fantasy]            9  \n",
      "4                   [Action, Horror, Sci-Fi]           13  \n",
      "...                                      ...          ...  \n",
      "447109                               [Drama]            7  \n",
      "447110               [Crime, Drama, Mystery]           10  \n",
      "447111                         [Documentary]            1  \n",
      "447112   [Action, Comedy, Mystery, Thriller]            4  \n",
      "447113                        [Crime, Drama]            2  \n",
      "\n",
      "[447114 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "#9) merge the result\n",
    "\n",
    "result['genres'] = result['genres'].str.split(', ')\n",
    "\n",
    "#Merge. Here I have the count of genres for each author\n",
    "merged_df = pd.merge(result.explode('genres'), genre_counts, on=['user_id', 'genres'], how='left')\n",
    "\n",
    "#merge. Here I have the \"genre_score\" for each film of each author.\n",
    "grouped_df = merged_df.groupby(['user_id', 'title', 'click_count']).agg({'genre_count': 'sum'}).reset_index()\n",
    "\n",
    "#merge\n",
    "result = pd.merge(result, grouped_df[['user_id', 'title', 'genre_count']], on=['user_id', 'title'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           user_id                       title  click_count  \\\n",
      "0       00004e2862                    Hannibal            1   \n",
      "1       000052a0a0                      Looper            9   \n",
      "10      000052a0a0               The Nice Guys            1   \n",
      "8       000052a0a0  Resident Evil: Retribution            1   \n",
      "5       000052a0a0                     Ant-Man            1   \n",
      "...            ...                         ...          ...   \n",
      "447101  fffeac83be                 Amanda Knox            2   \n",
      "447108  fffeac83be    The Paedophile Next Door            1   \n",
      "447112  ffff2c5f9e                    Hot Fuzz            1   \n",
      "447111  ffff2c5f9e           Forks Over Knives            1   \n",
      "447113  ffffd36adf            Dead Man Walking            1   \n",
      "\n",
      "                                            genres  genre_count  importance  \n",
      "0                         [Crime, Drama, Thriller]            3           4  \n",
      "1                [Action, Drama, Sci-Fi, Thriller]           18          27  \n",
      "10      [Action, Comedy, Crime, Mystery, Thriller]           20          21  \n",
      "8               [Action, Horror, Sci-Fi, Thriller]           19          20  \n",
      "5              [Action, Adventure, Comedy, Sci-Fi]           16          17  \n",
      "...                                            ...          ...         ...  \n",
      "447101                        [Documentary, Crime]            5           7  \n",
      "447108                               [Documentary]            3           4  \n",
      "447112         [Action, Comedy, Mystery, Thriller]            4           5  \n",
      "447111                               [Documentary]            1           2  \n",
      "447113                              [Crime, Drama]            2           3  \n",
      "\n",
      "[447114 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "#10) get the new score and sort the result\n",
    "import math\n",
    "#sum the clicks and the genres (final score)\n",
    "result['importance'] = result['click_count'] + result['genre_count']\n",
    "\n",
    "#sort the results \n",
    "result = result.sort_values(by=['user_id', 'importance'], ascending=[True, False])\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**comment**\n",
    "\n",
    "We can see that the results slightly changes. The top 10 movies are exactly the same as before because the prime filter is the *click-count* (which remains invariated). Yet, the final order is different as in this case we take into account also the genre preferences of a user. Taking as an example *000052a0a0* user, the top 10 of films between only films and both films and genre is different: <br>\n",
    "- **Looper** still dominates as it is the one with both most clicks and with genres that are liked (or at least seen) the most by the user. <br>\n",
    "- However, when we get to the second position, we can immediatly see a difference: for genre and clicks now **The Nice Guys** is the second best guess whereas on the original top list it was **Frailty**. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Minhash Signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "steps to follow: <br>\n",
    "1) **take the data**: <br>\n",
    "extract the data I need (\"user_id\" and \"genres\" columns)\n",
    "2) **store users**: <br>\n",
    "I put into a dictionary all genres associated with a user.\n",
    "3) **create hash function**: <br>\n",
    "I create the hash function by creating my own code as it is **not permitted** to use **already implemented functions**.\n",
    "4) **calculate the minhash**: <br>\n",
    "Now I find the minhas for genres and I store the values into a dictionary.\n",
    "5) **create the buckets**: <br>\n",
    "I create another dictionary in order to store users based on their minhash.Users with the same MinHash signature are grouped into the same bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) retrieve useful data\n",
    "data = dataset[[\"user_id\", \"genres\"]]\n",
    "\n",
    "data = data.sort_values(by=['user_id'], ascending=[True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2) store users\n",
    "\n",
    "#store in this dictionary the user, and the genre they watched (liked)\n",
    "user_sets = {}\n",
    "\n",
    "#for each row od the dataframe:\n",
    "for index, row in data.iterrows():\n",
    "    #take the value in user_id column \n",
    "    user_id = row['user_id']\n",
    "    #take the value(s) in genre column\n",
    "    genres = set(row['genres'].split(', '))\n",
    "    \n",
    "    if user_id not in user_sets:\n",
    "        user_sets[user_id] = genres\n",
    "    else:\n",
    "        #if the user_id already exist then append the genres\n",
    "        user_sets[user_id].update(genres)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3) create my hash function   \n",
    "\n",
    "def my_hash(input_string):\n",
    "    ##random number\n",
    "    hash_value = 13\n",
    "    \n",
    "    for char in input_string:\n",
    "        #other random number + univocate code for each character\n",
    "        hash_value = (hash_value * 17 + ord(char)) % (2**64)\n",
    "\n",
    "    return hash_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4) calculate minhash\n",
    "\n",
    "#store the minhash. Here I have the minhash and the users associated with it\n",
    "minhash_signatures = {}\n",
    "\n",
    "for user_id, genres_set in user_sets.items():\n",
    "    #store hash values for the genre\n",
    "    hash_values = []\n",
    "\n",
    "    #calculate hash value for genre\n",
    "    for genre in genres_set:\n",
    "        #append it\n",
    "        hash_values.append(my_hash(genre))\n",
    "\n",
    "    #search the minimun\n",
    "    minhash_value = min(hash_values)\n",
    "\n",
    "    #stor it for each user\n",
    "    minhash_signatures[user_id] = [minhash_value]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5) bucket\n",
    "#group similar users into the sam bucket\n",
    "buckets = {}\n",
    "\n",
    "# Iterate through users and their MinHash signatures\n",
    "for user_id, signature in minhash_signatures.items():\n",
    "    # Convert the signature list to a tuple to use it as a dictionary key\n",
    "    signature_key = tuple(signature)\n",
    "\n",
    "    # Add the user to the corresponding bucket\n",
    "    if signature_key in buckets:\n",
    "        buckets[signature_key].append(user_id)\n",
    "    else:\n",
    "        buckets[signature_key] = [user_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Locality-Sensitive Hashing (LSH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that your buckets are ready, it's time to ask a few queries. We will provide you with some user_ids and ask you to recommend at __most five movies__ to the user to watch based on the movies clicked by similar users. \n",
    "\n",
    "To recommend at most five movies given a user_id, use the following procedure: \n",
    "\n",
    "1. Identify the <ins>two most similar</ins> users to this user.\n",
    "2. If these two users have any movies __in common__, recommend those movies based on the total number of clicks by these users.\n",
    "3. If there are __no more common__ movies, try to propose the most clicked movies by the __most similar user first__, followed by the other user. \n",
    "\n",
    "__Note:__ At the end of the process, we expect to see at most five movies recommended to the user.\n",
    "\n",
    "__Example:__ assume you've identified user __A__ and __B__ as the most similar users to a single user, and we have the following records on these users: \n",
    "\n",
    "- User A with 80% similarity\n",
    "- User B with 50% similarity\n",
    "  \n",
    "|user|movie title|#clicks|\n",
    "|---|---|---|\n",
    "|A|Wild Child|20|\n",
    "|A|Innocence|10|\n",
    "|A|Coin Heist|2|\n",
    "|B|Innocence|30|\n",
    "|B|Coin Heist|15|\n",
    "|B|Before I Fall|30|\n",
    "|B|Beyond Skyline|8|\n",
    "|B|The Amazing Spider-Man|5|\n",
    "\n",
    "- __Recommended movies__ in order:\n",
    "    - Innocence\n",
    "    - Coin Heist\n",
    "    - Wild Child\n",
    "    - Before I Fall\n",
    "    - Beyond Skyline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Grouping Users together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will deal with clustering algorithms that will provide groups of Netflix users that are similar among them.\n",
    "\n",
    "To solve this task, you must accomplish the following stages:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Getting your data + feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)  Access to the data found in [this dataset](https://www.kaggle.com/datasets/vodclickstream/netflix-audience-behaviour-uk-movies)\n",
    "\n",
    "2)  Sometimes, the features (variables, fields) are not given in a dataset but can be created from it; this is known as *feature engineering*. For example, the original dataset has several clicks done by the same user, so grouping data by user_id will allow you to create new features **for each** user:\n",
    "\n",
    "    a)  Favorite genre (i.e., the genre on which the user spent the most time)\n",
    "\n",
    "    b)  Average click duration\n",
    "\n",
    "    c)  Time of the day (Morning/Afternoon/Night) when the user spends the most time on the platform (the time spent is tracked through the duration of the clicks)\n",
    "\n",
    "    d)  Is the user an old movie lover, or is he into more recent stuff (content released after 2010)?\n",
    "\n",
    "    e)  Average time spent a day by the user (considering only the days he logs in)\n",
    "\n",
    "So, in the end, you should have for each user_id five features.\n",
    "\n",
    "3)  Consider at least 10 additional features that can be generated for each user_id (you can use chatGPT or other LLM tools for suggesting features to create). Describe each of them and add them to the previous dataset you made (the one with five features). In the end, you should have for each user at least 15 features (5 recommended + 10 suggested by you)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Choose your features (variables)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that you have plenty of features to work with now. So, it would be best to find a way to reduce the dimensionality (reduce the number of variables to work with). You can follow the subsequent directions to achieve it:\n",
    "\n",
    "1)  *To normalise or not to normalise? That's the question*. Sometimes, it is worth normalizing (scaling) the features. Explain if it is a good idea to perform any normalization method. If you think the normalization should be used, apply it to your data (look at the available normalization functions in the `scikit-learn` library).\n",
    "\n",
    "2)  Select **one** method for dimensionality reduction and apply it to your data. Some suggestions are Principal Component Analysis, Multiple Correspondence Analysis, Singular Value Decomposition, Factor Analysis for Mixed Data, Two-Steps clustering. Make sure that the method you choose applies to the features you have or modify your data to be able to use it. Explain why you chose that method and the limitations it may have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Clustering!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)  Implement the K-means clustering algorithm (**not** ++: random initialization) using MapReduce. We ask you to write the algorithm from scratch following what you learned in class.\n",
    "\n",
    "2)  Find an optimal number of clusters. Use at least two different methods. If your algorithms provide diverse optimal K's, select one of them and explain why you chose it.\n",
    "\n",
    "3)  Run the algorithm on the data obtained from the dimensionality reduction.\n",
    "\n",
    "4)  Implement **K-means++** from scratch and explain the differences with the results you got earlier.\n",
    "\n",
    "5)  Ask ChatGPT to recommend other clustering algorithms and choose one. Explain your choice, then ask ChatGPT to implement it or use already implemented versions (e.g., the one provided in the scikit-learn library) and run it on your data. Explain the differences (if there are any) in the results. Which one is the best, in your opinion, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Analysing your results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are often encouraged to explain the main characteristics that your clusters have. The latter is called the *Characterizing Clusters* step. Thus, follow the next steps to do it:\n",
    "\n",
    "1)  Select 2-3 variables you think are relevant to identify the cluster of the customer. For example, Time_Day, Average Click Duration, etc.\n",
    "\n",
    "2)  Most of your selected variables will be numerical (continuous or discrete), then categorize them into four categories.\n",
    "\n",
    "3)  With the selected variables, perform pivot tables. On the horizontal axis, you will have the clusters, and on the vertical axis, you will have the categories of each variable. Notice that you have to do one pivot table per variable.\n",
    "\n",
    "4)  Calculate the percentage by column for each pivot table. The sum of each row (cluster) must be 100.\n",
    "\n",
    "5)  Interpret the results for each pivot table.\n",
    "\n",
    "6)  Use any known metrics to estimate clustering algorithm performance (how good are the clusters you found?). Comment on the results obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bonus Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remind you that we consider and grade the bonuses only if you complete the entire assignment.\n",
    "\n",
    "[Density-based clustering](https://wires.onlinelibrary.wiley.com/doi/epdf/10.1002/widm.30) identifies clusters as regions in the data space with high point density that are separated from other clusters by regions of low point density. The data points in the separating regions of low point density are typically considered noise or outliers. Typical algorithms that fall into this category are [OPTICS](https://dl.acm.org/doi/pdf/10.1145/304181.304187) and [DBSCAN](https://cdn.aaai.org/KDD/1996/KDD96-037.pdf).\n",
    "\n",
    "1)  Ask ChatGPT (or any other LLM tool) to list three algorithms for Density-Based Clustering. Choose one and use it on the same dataset you used in 2.3. Analyze your results: how different are they from the centroid-based version?\n",
    "\n",
    "__Note__: You can implement your algorithm from scratch or use the one implemented in the scikit-learn library; the choice is up to you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Command Line Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is another command line question to enjoy. We previously stated that using the command line tools is a skill that Data Scientists must master.\n",
    "\n",
    "In this question, you should use any command line tool that you know to answer the following questions using the same dataset that you have been using so far:\n",
    "  + What is the most-watched Netflix title?\n",
    "  + Report the average time between subsequent clicks on Netflix.com\n",
    "  + Provide the ID of the user that has spent the most time on Netflix\n",
    "    \n",
    "__Important note:__ You may work on this question in any environment (AWS, your PC command line, Jupyter notebook, etc.), but the final script must be placed in CommandLine.sh, which must be executable. Please run the script and include a __screenshot__ of the <ins>output</ins> in the notebook for evaluation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Algorithmic Question "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Federico studies in a demanding university where he has to take a certain number $N$ of exams to graduate,  but he is free to choose in which order he will take these exams. Federico is panicking since this university is not only one of the toughest in the world but also one of the weirdest. His final grade won't depend at all on the mark he gets in these courses: there's a precise evaluation system. \n",
    "\n",
    "He was given an initial personal score of $S$ when he enrolled, which changes every time he takes an exam: now comes the crazy part.\n",
    "He soon discovered that every of the $N$ exams he has to take is assigned a mark $p$. Once he has chosen an exam, his score becomes equal to the mark $p$, and at the same time, the scoring system changes:\n",
    "+ If he takes an \"easy\" exam (the score of the exam being less than his score), every other exam's mark is increased by the quantity $S - p$.\n",
    "+ If he takes a \"hard\" exam (the score of the exam is greater than his score), every other exam's mark is decreased by the quantity $p - S$.\n",
    "  \n",
    "So, for example, consider $S=8$ as the initial personal score. Federico must decide which exam he wants to take, being $[5,7,1]$ the marks list. If he takes the first one, being $5 < 8$ and $8 - 5 = 3$, the remaining list now becomes $[10,4]$, and his score is updated as $S = 5$.\n",
    "\n",
    "In this chaotic university where the only real exam seems to be choosing the best way to take exams, you are the poor student advisor who is facing a long queue of confused people who need some help. Federico is next in line,  and he comes up in turn with an inescapable question: he wants to know which is the highest score possible he could get. \n",
    "\n",
    "a) Fortunately, you have a computer app designed by a brilliant student. Federico wants you to show him the code which this app is based on\n",
    "because he wants to do paid counseling for other desperate students: in a *recursive* fashion, the helped helps the helpable.\n",
    "\n",
    "b) Federico is getting angry because he claims that your code is slow! Show him formally with a big-O notation that he is as crazy as this university! \n",
    "\n",
    "c) If, unfortunately, Federico is right in the grip of madness, he will threaten you to optimize the code through a different approach. \n",
    "You should end this theater of the absurd by any means! (And again, formally prove that you improved time complexity)\n",
    "\n",
    "d) Ask chatGPT for a third (optimized) implementation and analyze again its time complexity. Be careful (and crafty) in defining the prompt, and challenge the machine in this coding question!\n",
    "\n",
    "Here are some input/output examples (the first value is the initial personal score, and the second line contains the list of marks): \n",
    "\n",
    "__Input 1__\n",
    "```\n",
    "8\n",
    "5 7 1 \n",
    "```\n",
    "\n",
    "__Output 1__\n",
    "```\n",
    "11\n",
    "```\n",
    "\n",
    "__Input 2__\n",
    "```\n",
    "25\n",
    "18 24 21 32 27\n",
    "```\n",
    "\n",
    "__Output 2__\n",
    "```\n",
    "44\n",
    "```\n",
    "\n",
    "__Input 3__\n",
    "```\n",
    "30\n",
    "13 27 41 59 28 33 39 19 52 48 55 79\n",
    "```\n",
    "\n",
    "__Output 3__\n",
    "```\n",
    "109\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
